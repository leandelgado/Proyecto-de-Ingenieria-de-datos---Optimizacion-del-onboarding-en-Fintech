# Proyecto de Ingenier√≠a de Datos: Optimizaci√≥n del Onboarding en Fintech

Este repositorio contiene la implementaci√≥n de un pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga) para un proyecto de Ingenier√≠a de Datos. [cite_start]El objetivo principal es mejorar el *engagement* de los usuarios con una aplicaci√≥n Fintech, centr√°ndose en la reducci√≥n de la p√©rdida de usuarios durante la etapa de *onboarding* inicial[cite: 31, 32]. [cite_start]La soluci√≥n aborda un problema de negocio real mediante el procesamiento y an√°lisis de datos a gran escala, utilizando una arquitectura Big Data distribuida[cite: 29, 62].

## üìù Descripci√≥n del Problema

[cite_start]Una Fintech con operaciones en Latinoam√©rica busca incrementar el *engagement* de sus usuarios para reducir la p√©rdida de usuarios registrados [cite: 31][cite_start], especialmente durante los primeros d√≠as de uso de la aplicaci√≥n[cite: 32]. [cite_start]Un an√°lisis previo sugiri√≥ que esta p√©rdida podr√≠a estar relacionada con la complejidad de la aplicaci√≥n y la alta carga cognitiva para los usuarios nuevos[cite: 33].

[cite_start]Para abordar esto, se propuso implementar una etapa de *onboarding* de 30 d√≠as de duraci√≥n [cite: 34][cite_start], guiando a los nuevos usuarios en la creaci√≥n de productos bancarios y reduciendo la carga cognitiva de la p√°gina de inicio durante este per√≠odo[cite: 34]. [cite_start]Para medir la efectividad de esta propuesta, se dise√±√≥ un experimento de tipo A/B Testing [cite: 35][cite_start], observando m√©tricas clave durante seis meses en usuarios de Brasil[cite: 36].

## üéØ Objetivos del Proyecto

* [cite_start]Desarrollar un proceso ETL robusto utilizando Apache Spark y Apache Cassandra[cite: 62].
* [cite_start]Implementar una arquitectura Big Data distribuida[cite: 62].
* [cite_start]Realizar una exhaustiva etapa de pre-procesamiento para asegurar la consistencia y calidad de los datos[cite: 64].
* [cite_start]Preparar los datos en una etapa de transformaci√≥n para el an√°lisis del desempe√±o de la etapa de *onboarding* en el grupo de tratamiento[cite: 66, 67].
* [cite_start]Calcular m√©tricas de negocio clave para evaluar el √©xito de la estrategia de *onboarding*[cite: 24].

## üèóÔ∏è Arquitectura Utilizada

[cite_start]Para este trabajo, se implement√≥ una arquitectura Big Data distribuida[cite: 1], aprovechando las siguientes tecnolog√≠as:

* [cite_start]**Apache Cassandra / DataStax**: Base de datos NoSQL orientada a columnas, escalable y distribuida[cite: 1]. [cite_start]Su rol en la arquitectura es el almacenamiento persistente de los datos[cite: 2]. [cite_start]DataStax, como plataforma empresarial basada en Cassandra, proporciona herramientas adicionales para gesti√≥n, seguridad y an√°lisis en tiempo real[cite: 4].
* [cite_start]**Apache Spark (PySpark)**: Motor de procesamiento distribuido[cite: 3, 6]. [cite_start]Se utiliz√≥ PySpark (la interfaz Python de Spark) para la transformaci√≥n, an√°lisis y limpieza de datos[cite: 3, 7]. [cite_start]Es fundamental para tareas de ETL (extracci√≥n, transformaci√≥n y carga), procesamiento por lotes, limpieza de datos, agregaciones y uniones[cite: 7].

![image](https://github.com/user-attachments/assets/5c328e1d-c472-466c-8a2b-232128795c10)

## üíª Tecnolog√≠as y Librer√≠as Utilizadas

* [cite_start]**Apache Cassandra**: Base de datos NoSQL distribuida [cite: 5][cite_start], utilizada como fuente y destino de los datos[cite: 5].
* [cite_start]**DataStax**: Base de datos en la nube [cite: 6] [cite_start](Astra) para el almacenamiento de datos limpios en la "zona Universal"[cite: 23].
* [cite_start]**Apache Spark**: Motor de procesamiento distribuido[cite: 6].
* [cite_start]**PySpark**: Interfaz Python para Apache Spark[cite: 6].
* [cite_start]**Pandas**: Librer√≠a Python utilizada[cite: 7].
* [cite_start]**NumPy**: Librer√≠a Python utilizada[cite: 7].
* [cite_start]**AstraPy**: Librer√≠a Python utilizada[cite: 7].
* [cite_start]**DateTime**: Librer√≠a Python utilizada[cite: 7].

## üìä Datasets Utilizados

Se proporcionaron extractos hist√≥ricos de los siguientes datasets:

* [cite_start]`dim_users`: Contiene informaci√≥n de usuarios en *onboarding* para una fecha determinada [cite: 48][cite_start], incluyendo `user_id` (identificador √∫nico de usuario con prefijo "MLB" para usuarios de Brasil) [cite: 51, 52] [cite_start]y `rubro` (un identificador de rubro de negocio para usuarios del segmento *seller*)[cite: 53].
* [cite_start]`fact_users_transactions`: Contiene todas las transacciones de los usuarios realizadas durante un per√≠odo determinado [cite: 49][cite_start], incluyendo `transaction_dt` (fecha de una transacci√≥n) [cite: 54][cite_start], `type` (identificador de tipo de transacci√≥n: 1-7 para pago, 8 y 9 para cobro) [cite: 54][cite_start], y `segment` (1 para *individuals*, 2 para *seller*)[cite: 55].
* [cite_start]`fact_users_onboarding`: Contiene la informaci√≥n del usuario de *onboarding* que permite calcular las m√©tricas de negocio para un per√≠odo determinado [cite: 50][cite_start], incluyendo `first_login_dt` (fecha hist√≥rica del primer inicio de sesi√≥n) [cite: 56][cite_start], `habito` (1 para presencia del atributo, 0 en caso contrario) [cite: 57][cite_start], `habito_dt` (fecha en que realiza el evento de h√°bito) [cite: 57][cite_start], `activacion` (1 para presencia del atributo, 0 en caso contrario) [cite: 57][cite_start], `activacion_dt` (fecha en que realiza el evento de activaci√≥n) [cite: 59][cite_start], `setup` (1 para presencia del atributo, 0 en caso contrario) [cite: 59][cite_start], `setup_dt` (fecha en que realiza el evento de setup) [cite: 60][cite_start], y `return` (1 si el usuario retorn√≥ posteriormente al *first login*)[cite: 61].

## üßπ Acciones de Pre-procesamiento y Transformaci√≥n

[cite_start]Se realiz√≥ una limpieza y transformaci√≥n exhaustiva de los datos, respetando siempre criterios basados en la l√≥gica de negocio[cite: 23, 71]. [cite_start]Este proceso asegur√≥ la consistencia de los datos, abordando valores perdidos e inconsistencias[cite: 64], para dejar las tablas `users`, `onboarding` y `transactions` listas para el an√°lisis. [cite_start]Los datos limpios fueron cargados en Astra, en la zona Universal[cite: 23].

## üìà Hallazgos y Resultados

[cite_start]Se utilizaron consultas SQL con PySpark sobre los datos limpios en la "zona Universal", permitiendo calcular m√©tricas clave que luego alimentaron la "zona Smart"[cite: 24].

[cite_start]Los resultados obtenidos fueron los siguientes[cite: 25]:

| M√©trica                 | Valor    |
| :---------------------- | :------- |
| **Drop Rate** | 29.00 %  |
| **Activation Rate** | 11.80 %  |
| **Setup Rate** | 44.60 %  |
| **Habito Individuals** | 4758     |
| **Habito Sellers** | 505      |

### Definiciones Clave:

* [cite_start]**Drop Rate**: Porcentaje de usuarios que no volvieron a usar la aplicaci√≥n despu√©s del registro[cite: 26, 39].
* [cite_start]**Activation Rate**: Porcentaje de usuarios en *onboarding* que realizaron al menos una primera acci√≥n transaccional[cite: 39]. [cite_start]Para este proyecto, se consideraron usuarios que realizaron al menos 5 transacciones[cite: 17].
* [cite_start]**Setup Rate**: Porcentaje de usuarios que realizaron al menos una acci√≥n de *setup* (por ejemplo, activar llave PIX, cargar tarjeta)[cite: 27, 47].
* [cite_start]**H√°bito Individuals**: Usuarios que realizan 5 transacciones en 5 d√≠as diferentes, durante el per√≠odo de *onboarding* (fijado en 30 d√≠as)[cite: 28, 44, 45].
* [cite_start]**H√°bito Sellers**: Usuarios que realizan 5 transacciones de cobro (sin importar los d√≠as), durante el per√≠odo de *onboarding* (fijado en 30 d√≠as)[cite: 28, 45, 46].

## ÁªìËÆ∫ Conclusi√≥n

Se ha implementado un proceso ETL integral y una arquitectura Big Data distribuida utilizando Apache Spark y Apache Cassandra. [cite_start]La limpieza y transformaci√≥n exhaustiva de los datos, guiadas por la l√≥gica de negocio, han permitido obtener un conjunto de datos consistente y listo para el an√°lisis[cite: 23]. Las m√©tricas calculadas proporcionan una base s√≥lida para evaluar el desempe√±o de la etapa de *onboarding* y tomar decisiones informadas para mejorar el *engagement* de los usuarios.

---
 
